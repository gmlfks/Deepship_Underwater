{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b080d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 109 190 237\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import glob\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tug_path = glob.glob('/data/hrjung/DeepShip_Raw/Tug/*.wav')\n",
    "cargo_path = glob.glob('/data/hrjung/DeepShip_Raw/Cargo/*.wav')\n",
    "passenger_path = glob.glob('/data/hrjung/DeepShip_Raw/Passengership/*.wav')\n",
    "tanker_path = glob.glob('/data/hrjung/DeepShip_Raw/Tanker/*.wav')\n",
    "\n",
    "print(len(tug_path), len(cargo_path), len(passenger_path), len(tanker_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd18d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def slice_path(path):\n",
    "    num_samples = len(path)\n",
    "    train_slice = int(num_samples * 0.7)\n",
    "    val_slice = int(num_samples * 0.2)\n",
    "    \n",
    "    train_path = path[:train_slice]\n",
    "    val_path = path[train_slice:train_slice + val_slice]\n",
    "    test_path = path[train_slice + val_slice:]\n",
    "    \n",
    "    return train_path, val_path, test_path\n",
    "\n",
    "\n",
    "def slice_audio(path):\n",
    "    arr = []\n",
    "    \n",
    "    for i in tqdm(range(len(path))):\n",
    "        audio, sr = torchaudio.load(path[i])\n",
    "        audio = audio.squeeze()\n",
    "        audio = audio/torch.max(audio)\n",
    "        \n",
    "        # 공백 영역을 제거\n",
    "        nonzero_indices = torch.nonzero(audio)\n",
    "        start_idx = nonzero_indices[0].item()\n",
    "        end_idx = nonzero_indices[-1].item()\n",
    "        audio = audio[start_idx:end_idx+1]\n",
    "        \n",
    "        # 5초 단위로 자름\n",
    "        sr_5s = sr * 5\n",
    "        for j in range(0, len(audio), sr_5s):\n",
    "            sliced_audio = audio[j:j+sr_5s]\n",
    "            # 만약 마지막 클립이 5초보다 짧다면 추가x\n",
    "            if len(sliced_audio) == sr_5s:\n",
    "                arr.append(sliced_audio)\n",
    "            \n",
    "    return np.array(arr)\n",
    "\n",
    "def extract_random_frames(audio_sample, num_samples=20480):\n",
    "    total_samples = len(audio_sample)\n",
    "\n",
    "    start_index = np.random.randint(0, total_samples - num_samples+1)\n",
    "    extracted_frame = audio_sample[start_index:start_index + num_samples]\n",
    "    \n",
    "    return extracted_frame\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, label, n_mfcc=60):\n",
    "        self.file_paths = file_paths\n",
    "        self.label = label\n",
    "        self.n_mfcc = n_mfcc\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio = self.file_paths[idx]\n",
    "        audio = np.array(audio)\n",
    "        extracted_frame = extract_random_frames(audio)\n",
    "        result = feature_extract.stack_features(waveform=extracted_frame)\n",
    "        result[1] = spec(result[1])\n",
    "        \n",
    "        label = int(self.label[idx])\n",
    "        \n",
    "        return result.float(), label\n",
    "\n",
    "def load_data():\n",
    "    # Load data paths as in the original code\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2004ff05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tug, val_tug, test_tug = slice_path(tug_path)\n",
    "train_cargo, val_cargo, test_cargo = slice_path(cargo_path)\n",
    "train_passenger, val_passenger, test_passenger = slice_path(passenger_path)\n",
    "train_tanker, val_tanker, test_tanker = slice_path(tanker_path)\n",
    "#각각의 클래스에서 trian, val, test로 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507591c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:14<00:00,  3.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76/76 [00:13<00:00,  5.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:03<00:00,  6.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00,  6.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 133/133 [00:13<00:00,  9.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  7.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  7.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165/165 [00:13<00:00, 12.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.23it/s]\n",
      " 56%|███████████████████████████████████████████████████████████████████▏                                                    | 14/25 [00:01<00:01,  8.26it/s]"
     ]
    }
   ],
   "source": [
    "sliced_tug_train = slice(train_tug)\n",
    "sliced_tug_val = slice(val_tug)\n",
    "sliced_tug_test = slice(test_tug)\n",
    "\n",
    "sliced_cargo_train = slice(train_cargo)\n",
    "sliced_cargo_val = slice(val_cargo)\n",
    "sliced_cargo_test = slice(test_cargo)\n",
    "\n",
    "sliced_passenger_train = slice(train_passenger)\n",
    "sliced_passenger_val = slice(val_passenger)\n",
    "sliced_passenger_test = slice(test_passenger)\n",
    "\n",
    "sliced_tanker_train = slice(train_tanker)\n",
    "sliced_tanker_val = slice(val_tanker)\n",
    "sliced_tanker_test = slice(test_tanker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sliced_audio = np.concatenate((np.array(sliced_tug_train), np.array(sliced_cargo_train), np.array(sliced_passenger_train), np.array(sliced_tanker_train)), axis= 0)\n",
    "val_sliced_audio = np.concatenate((np.array(sliced_tug_val), np.array(sliced_cargo_val), np.array(sliced_passenger_val), np.array(sliced_tanker_val)), axis= 0)\n",
    "test_sliced_audio = np.concatenate((np.array(sliced_tug_test), np.array(sliced_cargo_test), np.array(sliced_passenger_test), np.array(sliced_tanker_test)), axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_tug_label = np.ones(len(sliced_tug_train)) * 0\n",
    "sliced_cargo_label = np.ones(len(sliced_cargo_train)) * 1 \n",
    "sliced_passenger_label = np.ones(len(sliced_passenger_train)) *2\n",
    "sliced_tanker_label = np.ones(len(sliced_tanker_train)) * 3\n",
    "\n",
    "train_label = np.concatenate((np.array(sliced_tug_label), np.array(sliced_cargo_label), np.array(sliced_passenger_label), np.array(sliced_tanker_label)), axis= 0)\n",
    "\n",
    "sliced_tug_label = np.ones(len(sliced_tug_val)) * 0\n",
    "sliced_cargo_label = np.ones(len(sliced_cargo_val)) * 1 \n",
    "sliced_passenger_label = np.ones(len(sliced_passenger_val)) *2\n",
    "sliced_tanker_label = np.ones(len(sliced_tanker_val)) * 3\n",
    "\n",
    "val_label = np.concatenate((np.array(sliced_tug_label), np.array(sliced_cargo_label), np.array(sliced_passenger_label), np.array(sliced_tanker_label)), axis= 0)\n",
    "\n",
    "sliced_tug_label = np.ones(len(sliced_tug_test)) * 0\n",
    "sliced_cargo_label = np.ones(len(sliced_cargo_test)) * 1 \n",
    "sliced_passenger_label = np.ones(len(sliced_passenger_test)) *2\n",
    "sliced_tanker_label = np.ones(len(sliced_tanker_test)) * 3\n",
    "\n",
    "test_label = np.concatenate((np.array(sliced_tug_label), np.array(sliced_cargo_label), np.array(sliced_passenger_label), np.array(sliced_tanker_label)), axis= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07464b45",
   "metadata": {},
   "source": [
    "# shuffle all segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_audio = np.concatenate((train_sliced_audio,val_sliced_audio,test_sliced_audio), axis= 0)\n",
    "sliced_label = np.concatenate((train_label,val_label,test_label), axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_random_frames(audio_sample, num_samples=20480):\n",
    "#     total_samples = len(audio_sample)\n",
    "\n",
    "#     start_index = np.random.randint(0, total_samples - num_samples+1)\n",
    "#     extracted_frame = audio_sample[start_index:start_index + num_samples]\n",
    "    \n",
    "#     return extracted_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6229c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "    def __init__(self):\n",
    "        self.n_mfcc = 60\n",
    "        self.n_mels = 60\n",
    "        self.sr = 22050\n",
    "\n",
    "    def extract_mfcc(self, waveform):\n",
    "        mfcc_feature = librosa.feature.mfcc(y=waveform, sr=self.sr, n_mfcc=self.n_mfcc, hop_length=512)\n",
    "        return torch.tensor(mfcc_feature)   \n",
    "\n",
    "    def extract_log_mel(self, waveform):\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=self.sr, n_mels=self.n_mels, hop_length=512)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        return torch.tensor(log_mel_spectrogram)\n",
    "\n",
    "    def extract_cctz(self, waveform):\n",
    "        chroma = librosa.feature.chroma_stft(y=waveform, sr=self.sr, hop_length=512)\n",
    "        contrast = librosa.feature.spectral_contrast(y=waveform, sr=self.sr, hop_length=512)\n",
    "        tonnetz = librosa.feature.tonnetz(y=waveform, sr=self.sr, hop_length=512)\n",
    "        zero_cross_rate = librosa.feature.zero_crossing_rate(waveform, hop_length=512)\n",
    "\n",
    "        chroma_tensor = torch.tensor(chroma)\n",
    "        contrast_tensor = torch.tensor(contrast)\n",
    "        tonnetz_tensor = torch.tensor(tonnetz)\n",
    "        zero_cross_rate_tensor = torch.tensor(zero_cross_rate)\n",
    "\n",
    "        cctz_features = torch.cat([chroma_tensor, contrast_tensor, tonnetz_tensor, zero_cross_rate_tensor], dim=0)\n",
    "        return cctz_features\n",
    "\n",
    "    def stack_features(self, waveform):\n",
    "        mfcc_feature = self.extract_mfcc(waveform=waveform)\n",
    "        log_mel_feature = self.extract_log_mel(waveform=waveform)\n",
    "        cctz_feature = self.extract_cctz(waveform=waveform)\n",
    "        \n",
    "        # Find the maximum feature dimension\n",
    "        max_feature_dim = max(mfcc_feature.size(0), log_mel_feature.size(0), cctz_feature.size(0))\n",
    "    \n",
    "        # Centered zero padding for feature dimension alignment\n",
    "        mfcc_tensor = torch.nn.functional.pad(mfcc_feature, (0, 0, (max_feature_dim - mfcc_feature.size(0)) // 2, (max_feature_dim - mfcc_feature.size(0) + 1) // 2))\n",
    "        log_mel_tensor = torch.nn.functional.pad(log_mel_feature, (0, 0, (max_feature_dim - log_mel_feature.size(0)) // 2, (max_feature_dim - log_mel_feature.size(0) + 1) // 2))\n",
    "        cctz_tensor = torch.nn.functional.pad(cctz_feature, (0, 0, (max_feature_dim - cctz_feature.size(0)) // 2, (max_feature_dim - cctz_feature.size(0) + 1) // 2))\n",
    "        \n",
    "        # pdb.set_trace()\n",
    "        # Stack the features along a new dimension\n",
    "        stacked_features = torch.stack([mfcc_tensor, log_mel_tensor, cctz_tensor], dim=0)\n",
    "        #stacked_features = torch.stack([mfcc_tensor], dim=0)\n",
    "        return stacked_features\n",
    "    \n",
    "feature_extract = preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fb01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spec_transform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(spec_transform,self).__init__()\n",
    "        sr = 22050\n",
    "        \n",
    "        self.time = AT.TimeMasking(time_mask_param=3)\n",
    "        self.freq = AT.FrequencyMasking(freq_mask_param=5)\n",
    "        \n",
    "    def forward(self, spec):\n",
    "        spec = self.time(spec)\n",
    "        spec = self.freq(spec)\n",
    "        return spec\n",
    "    \n",
    "spec = spec_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b284ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_samples = len(sliced_audio)\n",
    "x = np.arange(num_total_samples)\n",
    "np.random.shuffle(x)\n",
    "\n",
    "num_train_samples = int(num_total_samples * 0.7)\n",
    "num_val_samples = int(num_total_samples * 0.2)\n",
    "\n",
    "train_x = x[:num_train_samples]\n",
    "val_x = x[num_train_samples:num_train_samples+num_val_samples]\n",
    "test_x = x[num_train_samples+num_val_samples:]\n",
    "\n",
    "train_file_path = [sliced_audio[i] for i in train_x]\n",
    "val_file_path = [sliced_audio[i] for i in val_x]\n",
    "test_file_path = [sliced_audio[i] for i in test_x]\n",
    "\n",
    "train_file_label = [sliced_label[i] for i in train_x]\n",
    "val_file_label = [sliced_label[i] for i in val_x]\n",
    "test_file_label = [sliced_label[i] for i in test_x]\n",
    "#Dataset\n",
    "train_dataset = AudioDataset(train_file_path, train_file_label)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=10)\n",
    "\n",
    "val_dataset = AudioDataset(val_file_path, val_file_label)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=True, num_workers=10)\n",
    "\n",
    "test_dataset = AudioDataset(test_file_path, test_file_label)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baec58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb664d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset),len(val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
