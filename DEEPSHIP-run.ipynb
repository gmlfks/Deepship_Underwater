{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import glob\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define paths\n",
    "tug_path = glob.glob('/data/hrjung/DeepShip_Raw/Tug/*.wav')\n",
    "cargo_path = glob.glob('/data/hrjung/DeepShip_Raw/Cargo/*.wav')\n",
    "passenger_path = glob.glob('/data/hrjung/DeepShip_Raw/Passengership/*.wav')\n",
    "tanker_path = glob.glob('/data/hrjung/DeepShip_Raw/Tanker/*.wav')\n",
    "\n",
    "# Print number of files in each category\n",
    "print(len(tug_path), len(cargo_path), len(passenger_path), len(tanker_path))\n",
    "\n",
    "# Set device for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Function to slice paths into training, validation, and test sets\n",
    "def slice_path(path):\n",
    "    num_samples = len(path)\n",
    "    train_slice = int(num_samples * 0.7)\n",
    "    val_slice = int(num_samples * 0.2)\n",
    "    \n",
    "    train_path = path[:train_slice]\n",
    "    val_path = path[train_slice:train_slice + val_slice]\n",
    "    test_path = path[train_slice + val_slice:]\n",
    "    \n",
    "    return train_path, val_path, test_path\n",
    "\n",
    "# Slice the paths into training, validation, and test sets for each category\n",
    "train_tug, val_tug, test_tug = slice_path(tug_path)\n",
    "train_cargo, val_cargo, test_cargo = slice_path(cargo_path)\n",
    "train_passenger, val_passenger, test_passenger = slice_path(passenger_path)\n",
    "train_tanker, val_tanker, test_tanker = slice_path(tanker_path)\n",
    "\n",
    "# Function to load and slice audio files into 5-second clips\n",
    "from tqdm import tqdm\n",
    "\n",
    "def slice(path):\n",
    "    arr = []\n",
    "    \n",
    "    for i in tqdm(range(len(path))):\n",
    "        audio, sr = torchaudio.load(path[i])\n",
    "        audio = audio.squeeze()\n",
    "        audio = audio / torch.max(audio)\n",
    "        \n",
    "        # Remove silent sections\n",
    "        nonzero_indices = torch.nonzero(audio)\n",
    "        start_idx = nonzero_indices[0].item()\n",
    "        end_idx = nonzero_indices[-1].item()\n",
    "        audio = audio[start_idx:end_idx+1]\n",
    "        \n",
    "        # Slice the audio into 5-second segments\n",
    "        sr_5s = sr * 5\n",
    "        for j in range(0, len(audio), sr_5s):\n",
    "            sliced_audio = audio[j:j+sr_5s]\n",
    "            # If the last segment is shorter than 5 seconds, skip it\n",
    "            if len(sliced_audio) == sr_5s:\n",
    "                arr.append(sliced_audio)\n",
    "    \n",
    "    return np.array(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the audio data for each category\n",
    "sliced_tug_train = slice(train_tug)\n",
    "sliced_tug_val = slice(val_tug)\n",
    "sliced_tug_test = slice(test_tug)\n",
    "\n",
    "sliced_cargo_train = slice(train_cargo)\n",
    "sliced_cargo_val = slice(val_cargo)\n",
    "sliced_cargo_test = slice(test_cargo)\n",
    "\n",
    "sliced_passenger_train = slice(train_passenger)\n",
    "sliced_passenger_val = slice(val_passenger)\n",
    "sliced_passenger_test = slice(test_passenger)\n",
    "\n",
    "sliced_tanker_train = slice(train_tanker)\n",
    "sliced_tanker_val = slice(val_tanker)\n",
    "sliced_tanker_test = slice(test_tanker)\n",
    "\n",
    "# Concatenate sliced audio data into train, validation, and test sets\n",
    "train_sliced_audio = np.concatenate(\n",
    "    (np.array(sliced_tug_train), np.array(sliced_cargo_train), np.array(sliced_passenger_train), np.array(sliced_tanker_train)),\n",
    "    axis=0\n",
    ")\n",
    "val_sliced_audio = np.concatenate(\n",
    "    (np.array(sliced_tug_val), np.array(sliced_cargo_val), np.array(sliced_passenger_val), np.array(sliced_tanker_val)),\n",
    "    axis=0\n",
    ")\n",
    "test_sliced_audio = np.concatenate(\n",
    "    (np.array(sliced_tug_test), np.array(sliced_cargo_test), np.array(sliced_passenger_test), np.array(sliced_tanker_test)),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# Create labels for each category: 0 for Tug, 1 for Cargo, 2 for Passenger, 3 for Tanker\n",
    "sliced_tug_label = np.ones(len(sliced_tug_train)) * 0\n",
    "sliced_cargo_label = np.ones(len(sliced_cargo_train)) * 1\n",
    "sliced_passenger_label = np.ones(len(sliced_passenger_train)) * 2\n",
    "sliced_tanker_label = np.ones(len(sliced_tanker_train)) * 3\n",
    "\n",
    "train_label = np.concatenate(\n",
    "    (np.array(sliced_tug_label), np.array(sliced_cargo_label), np.array(sliced_passenger_label), np.array(sliced_tanker_label)),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# Labels for validation set\n",
    "sliced_tug_label = np.ones(len(sliced_tug_val)) * 0\n",
    "sliced_cargo_label = np.ones(len(sliced_cargo_val)) * 1\n",
    "sliced_passenger_label = np.ones(len(sliced_passenger_val)) * 2\n",
    "sliced_tanker_label = np.ones(len(sliced_tanker_val)) * 3\n",
    "\n",
    "val_label = np.concatenate(\n",
    "    (np.array(sliced_tug_label), np.array(sliced_cargo_label), np.array(sliced_passenger_label), np.array(sliced_tanker_label)),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# Labels for test set\n",
    "sliced_tug_label = np.ones(len(sliced_tug_test)) * 0\n",
    "sliced_cargo_label = np.ones(len(sliced_cargo_test)) * 1\n",
    "sliced_passenger_label = np.ones(len(sliced_passenger_test)) * 2\n",
    "sliced_tanker_label = np.ones(len(sliced_tanker_test)) * 3\n",
    "\n",
    "test_label = np.concatenate(\n",
    "    (np.array(sliced_tug_label), np.array(sliced_cargo_label), np.array(sliced_passenger_label), np.array(sliced_tanker_label)),\n",
    "    axis=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 세그먼트를 하나로 합쳐서 처리\n",
    "sliced_audio = np.concatenate((train_sliced_audio, val_sliced_audio, test_sliced_audio), axis=0)\n",
    "sliced_label = np.concatenate((train_label, val_label, test_label), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 2: 랜덤 프레임 추출 함수 정의\n",
    "def extract_random_frames(audio_sample, num_samples=20480):\n",
    "    total_samples = len(audio_sample)\n",
    "    start_index = np.random.randint(0, total_samples - num_samples + 1)\n",
    "    extracted_frame = audio_sample[start_index:start_index + num_samples]\n",
    "    \n",
    "    return extracted_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.n_mfcc = 60  # MFCC 차원 수\n",
    "        self.n_mels = 60  # Mel 스펙트로그램 차원 수\n",
    "        self.sr = 22050    # 샘플링 주파수\n",
    "\n",
    "    def extract_mfcc(self, waveform):\n",
    "        mfcc_feature = librosa.feature.mfcc(y=waveform, sr=self.sr, n_mfcc=self.n_mfcc, hop_length=512)\n",
    "        return torch.tensor(mfcc_feature)   \n",
    "\n",
    "    def extract_log_mel(self, waveform):\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=self.sr, n_mels=self.n_mels, hop_length=512)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        return torch.tensor(log_mel_spectrogram)\n",
    "\n",
    "    def extract_cctz(self, waveform):\n",
    "        chroma = librosa.feature.chroma_stft(y=waveform, sr=self.sr, hop_length=512)\n",
    "        contrast = librosa.feature.spectral_contrast(y=waveform, sr=self.sr, hop_length=512)\n",
    "        tonnetz = librosa.feature.tonnetz(y=waveform, sr=self.sr, hop_length=512)\n",
    "        zero_cross_rate = librosa.feature.zero_crossing_rate(waveform, hop_length=512)\n",
    "\n",
    "        chroma_tensor = torch.tensor(chroma)\n",
    "        contrast_tensor = torch.tensor(contrast)\n",
    "        tonnetz_tensor = torch.tensor(tonnetz)\n",
    "        zero_cross_rate_tensor = torch.tensor(zero_cross_rate)\n",
    "\n",
    "        cctz_features = torch.cat([chroma_tensor, contrast_tensor, tonnetz_tensor, zero_cross_rate_tensor], dim=0)\n",
    "        return cctz_features\n",
    "\n",
    "    def stack_features(self, waveform):\n",
    "        mfcc_feature = self.extract_mfcc(waveform=waveform)\n",
    "        log_mel_feature = self.extract_log_mel(waveform=waveform)\n",
    "        cctz_feature = self.extract_cctz(waveform=waveform)\n",
    "        \n",
    "        # 가장 큰 feature dimension을 찾습니다.\n",
    "        max_feature_dim = max(mfcc_feature.size(0), log_mel_feature.size(0), cctz_feature.size(0))\n",
    "\n",
    "        # feature 차원 정렬을 위한 0-padding (중앙 정렬)\n",
    "        mfcc_tensor = torch.nn.functional.pad(mfcc_feature, (0, 0, (max_feature_dim - mfcc_feature.size(0)) // 2, (max_feature_dim - mfcc_feature.size(0) + 1) // 2))\n",
    "        log_mel_tensor = torch.nn.functional.pad(log_mel_feature, (0, 0, (max_feature_dim - log_mel_feature.size(0)) // 2, (max_feature_dim - log_mel_feature.size(0) + 1) // 2))\n",
    "        cctz_tensor = torch.nn.functional.pad(cctz_feature, (0, 0, (max_feature_dim - cctz_feature.size(0)) // 2, (max_feature_dim - cctz_feature.size(0) + 1) // 2))\n",
    "        \n",
    "        # feature들을 새로운 차원으로 스택\n",
    "        stacked_features = torch.stack([mfcc_tensor, log_mel_tensor, cctz_tensor], dim=0)\n",
    "        return stacked_features\n",
    "\n",
    "# 전처리 객체 생성\n",
    "feature_extract = Preprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as AT\n",
    "\n",
    "class SpecTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpecTransform, self).__init__()\n",
    "        sr = 22050  # 샘플링 주파수 설정\n",
    "        \n",
    "        # 시간 마스킹과 주파수 마스킹 변환 정의\n",
    "        self.time = AT.TimeMasking(time_mask_param=3)  # 시간 마스크 크기 3\n",
    "        self.freq = AT.FrequencyMasking(freq_mask_param=5)  # 주파수 마스크 크기 5\n",
    "\n",
    "    def forward(self, spec):\n",
    "        # 스펙트로그램에 시간 마스크와 주파수 마스크를 적용\n",
    "        spec = self.time(spec)\n",
    "        spec = self.freq(spec)\n",
    "        return spec\n",
    "\n",
    "# 변환 객체 생성\n",
    "spec = SpecTransform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오디오 데이터셋 클래스 정의\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, label, n_mfcc=60):\n",
    "        self.file_paths = file_paths  # 오디오 파일 경로\n",
    "        self.label = label  # 레이블\n",
    "        self.n_mfcc = n_mfcc  # MFCC 차원 설정\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)  # 데이터셋 크기 반환\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스의 오디오 파일을 가져와서 랜덤 프레임을 추출\n",
    "        audio = self.file_paths[idx]\n",
    "        audio = np.array(audio)\n",
    "        extracted_frame = extract_random_frames(audio)  # 랜덤 프레임 추출\n",
    "        \n",
    "        # 특징 추출\n",
    "        result = feature_extract.stack_features(waveform=extracted_frame)\n",
    "        \n",
    "        # 스펙트로그램에 변환 적용\n",
    "        result[1] = spec(result[1])\n",
    "        \n",
    "        # 레이블 반환\n",
    "        label = int(self.label[idx])\n",
    "        return result.float(), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할 및 DataLoader 설정\n",
    "num_total_samples = len(sliced_audio)  # 총 샘플 수\n",
    "x = np.arange(num_total_samples)  # 샘플 인덱스 배열 생성\n",
    "np.random.shuffle(x)  # 인덱스 랜덤 섞기\n",
    "\n",
    "# 훈련, 검증, 테스트 데이터의 비율 설정\n",
    "num_train_samples = int(num_total_samples * 0.7)\n",
    "num_val_samples = int(num_total_samples * 0.2)\n",
    "\n",
    "# 훈련, 검증, 테스트 데이터셋 분할\n",
    "train_x = x[:num_train_samples]\n",
    "val_x = x[num_train_samples:num_train_samples+num_val_samples]\n",
    "test_x = x[num_train_samples+num_val_samples:]\n",
    "\n",
    "# 파일 경로 및 레이블 분할\n",
    "train_file_path = [sliced_audio[i] for i in train_x]\n",
    "val_file_path = [sliced_audio[i] for i in val_x]\n",
    "test_file_path = [sliced_audio[i] for i in test_x]\n",
    "\n",
    "train_file_label = [sliced_label[i] for i in train_x]\n",
    "val_file_label = [sliced_label[i] for i in val_x]\n",
    "test_file_label = [sliced_label[i] for i in test_x]\n",
    "\n",
    "# Dataset 객체 생성\n",
    "train_dataset = AudioDataset(train_file_path, train_file_label)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=10)\n",
    "\n",
    "val_dataset = AudioDataset(val_file_path, val_file_label)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=True, num_workers=10)\n",
    "\n",
    "test_dataset = AudioDataset(test_file_path, test_file_label)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 크기 확인\n",
    "train_dataset[0][0].shape  # 첫 번째 샘플의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 검증 데이터셋 크기 확인\n",
    "len(train_dataset), len(val_dataset)  # 훈련 및 검증 데이터셋의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        \n",
    "        # 평균 풀링과 최대 풀링을 사용하여 채널 중요도 계산\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        # 채널 수를 줄였다가 다시 확장하는 두 개의 1x1 컨볼루션을 사용\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 평균 풀링과 최대 풀링을 통해 각 채널에 대한 중요도를 계산\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        \n",
    "        # 각 채널의 중요도를 합산하고 입력에 가중치를 적용\n",
    "        out = avg_out + max_out\n",
    "        out = out * x\n",
    "        return F.relu(out)  # ReLU 활성화 함수 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage1, self).__init__()\n",
    "\n",
    "        # 첫 번째 컨볼루션 + 배치 정규화 + GELU 활성화 함수\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # 채널 어텐션\n",
    "        self.cam1 = ChannelAttention(64)\n",
    "        # 맥스풀링\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 첫 번째 컨볼루션 및 채널 어텐션 적용\n",
    "        out = self.conv1(x)\n",
    "        out = self.cam1(out)\n",
    "        out = self.maxpool(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(AConvBlock, self).__init__()\n",
    "\n",
    "        # 메인 브랜치: 여러 개의 7x7 컨볼루션과 배치 정규화\n",
    "        self.main_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=stride, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels * 8, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(out_channels * 8)\n",
    "        )\n",
    "        \n",
    "        # Shortcut: 입력과 출력의 차원이 다르면 1x1 컨볼루션을 사용해 맞춰줌\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * 8:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * 8, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * 8)\n",
    "            )\n",
    "        \n",
    "        # 채널 어텐션\n",
    "        self.cam = ChannelAttention(out_channels * 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 메인 브랜치를 통과한 결과\n",
    "        out = self.main_branch(x)\n",
    "        # shortcut 연결 추가\n",
    "        out += self.cam(self.shortcut(x))\n",
    "        return F.relu(out)  # ReLU 활성화 함수 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # 네트워크의 각 단계 정의\n",
    "        self.conv1_x = Stage1()\n",
    "        self.conv2_x = AConvBlock(64, 32)  # 입력 채널 64, 출력 채널 32\n",
    "        self.conv3_x = AConvBlock(32*8, 32)\n",
    "        self.conv4_x = AConvBlock(32*8, 32)\n",
    "        self.conv5_x = AConvBlock(32*8, 64)  # 입력 채널 32*8, 출력 채널 64\n",
    "        self.conv6_x = AConvBlock(64*8, 64)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64*8, 24)\n",
    "        self.fc2 = nn.Linear(24, num_classes)\n",
    "\n",
    "        # Average pooling 및 PReLU\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_x(x)\n",
    "        out = self.conv2_x(out)\n",
    "        out = self.conv3_x(out)\n",
    "        out = self.conv4_x(out)\n",
    "        out = self.conv5_x(out)\n",
    "        out = self.conv6_x(out)\n",
    "        \n",
    "        # Average pooling 후 flatten\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        x = self.prelu(out)\n",
    "        y = self.fc2(x)\n",
    "        \n",
    "        return x, y  # x는 중간 출력, y는 최종 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet().to(device)  # 모델을 지정된 장치로 이동\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Center loss.\n",
    "    \n",
    "    Reference:\n",
    "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, feat_dim=256, use_gpu=True):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        # 클래스 중심 초기화\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 각 클래스 중심과의 거리 계산\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        # 레이블과 일치하는 위치 마스크 생성\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: \n",
    "            classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        # 거리 계산 후 손실 값 구하기\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "# 손실 함수와 CenterLoss 초기화\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # CrossEntropyLoss는 분류 문제에서 사용\n",
    "center = CenterLoss(4, 24).to(device)  # 4개의 클래스, feature dimension 24\n",
    "\n",
    "# 옵티마이저 설정\n",
    "opti1 = Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)  # 모델 파라미터에 대해 Adam 옵티마이저 사용\n",
    "opti2 = SGD(center.parameters(), lr=0.5)  # CenterLoss 파라미터에 대해 SGD 사용\n",
    "\n",
    "# 학습률 스케줄러 설정\n",
    "scheduler1 = torch.optim.lr_scheduler.StepLR(opti1, step_size=10, gamma=0.5)  # 10번째 epoch마다 lr을 0.5배로 감소\n",
    "scheduler2 = torch.optim.lr_scheduler.StepLR(opti2, step_size=10, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, data_len, opti1, opti2):\n",
    "    correct = 0\n",
    "    losses = 0\n",
    "\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for data, target in tqdm(dataloader):  # 배치 단위로 데이터를 순차적으로 가져옴\n",
    "        data = data.to(device)  # 데이터를 device(CPU/GPU)로 이동\n",
    "        target = target.to(device)  # 타겟 레이블을 device로 이동\n",
    "        \n",
    "        cen, output = model(data)  # 모델의 출력값과 특징 벡터 계산\n",
    "        loss1 = criterion(output, target)  # CrossEntropyLoss 계산\n",
    "        loss2 = center(cen, target)  # CenterLoss 계산\n",
    "        loss = loss1 + loss2  # 두 손실을 합산\n",
    "\n",
    "        opti1.zero_grad()  # 옵티마이저1의 기울기를 0으로 초기화\n",
    "        opti2.zero_grad()  # 옵티마이저2의 기울기를 0으로 초기화\n",
    "        loss.backward()  # 역전파 계산\n",
    "        opti1.step()  # 옵티마이저1 파라미터 업데이트\n",
    "        opti2.step()  # 옵티마이저2 파라미터 업데이트\n",
    "\n",
    "        # 예측값을 구하고 정확도 계산\n",
    "        pred = output.max(1, keepdim=True)[1]  # 예측값의 인덱스 추출\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()  # 정확도 계산\n",
    "        losses += loss.item()  # 손실 값 합산\n",
    "\n",
    "    # 학습률 스케줄러를 업데이트\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "\n",
    "    # 정확도와 평균 손실 반환\n",
    "    return 100 * correct / data_len, losses / data_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, data_len):\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    with torch.no_grad():  # 평가 시 기울기 계산을 하지 않음\n",
    "        for data, target in dataloader:  # 데이터 로더에서 배치 단위로 데이터를 가져옴\n",
    "            data = data.to(device)  # 데이터를 device로 이동\n",
    "            target = target.to(device)  # 타겟 레이블을 device로 이동\n",
    "\n",
    "            _, output = model(data)  # 모델 출력 계산\n",
    "            loss = criterion(output, target)  # 손실 계산\n",
    "\n",
    "            # 예측값을 구하고 정확도 계산\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = 100. * correct / data_len\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 300\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    # Training the model\n",
    "    train_acc, train_loss = train(model, train_dataloader, criterion, len(train_dataloader.dataset), opti1, opti2)\n",
    "    \n",
    "    # Evaluating the model on validation data\n",
    "    val_acc = evaluate(model, val_dataloader, criterion, len(val_dataloader.dataset))\n",
    "    \n",
    "    # Uncomment the line below if you want to evaluate on test data\n",
    "    # test_acc = evaluate(model, test_dataloader, criterion, len(test_dataloader.dataset))\n",
    "\n",
    "    # Storing the accuracies\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Printing the results for the current epoch\n",
    "    print(f\"[Epoch: {i+1}], [Validation Acc: {val_acc:.4f}]\")\n",
    "    print(f\"train_acc: {train_acc}, train_loss: {train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Plotting training and validation accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracies')\n",
    "plt.show()\n",
    "\n",
    "# Setting up the device and loading the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet().to(device)\n",
    "model.load_state_dict(torch.load('./deepship7.pt'))\n",
    "\n",
    "# Function to evaluate precision, recall, and F1-score\n",
    "def evaluate_metrics(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            _, output = model(data)\n",
    "            pred = output.argmax(dim=1)  # Get the class with the highest score\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for each class\n",
    "    precision = precision_score(all_targets, all_preds, average=None)\n",
    "    recall = recall_score(all_targets, all_preds, average=None)\n",
    "    f1 = f1_score(all_targets, all_preds, average=None)\n",
    "\n",
    "    # Calculate average precision, recall, and F1-score\n",
    "    avg_precision = precision.mean()\n",
    "    avg_recall = recall.mean()\n",
    "    avg_f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1, avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Evaluate metrics on the test dataset\n",
    "class_precision, class_recall, class_f1, avg_precision, avg_recall, avg_f1 = evaluate_metrics(model, test_dataloader)\n",
    "\n",
    "# Print performance for each class\n",
    "for i in range(len(class_precision)):\n",
    "    print(f\"Class {i} - Precision: {class_precision[i]:.4f}, Recall: {class_recall[i]:.4f}, F1-score: {class_f1[i]:.4f}\")\n",
    "\n",
    "# Print average performance across all classes\n",
    "print(f\"Avg Precision: {avg_precision:.4f}, Avg Recall: {avg_recall:.4f}, Avg F1-score: {avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class 0 - Precision: 0.9922, Recall: 0.9821, F1-score: 0.9871\n",
    "#Class 1 - Precision: 0.9683, Recall: 0.9532, F1-score: 0.9607\n",
    "#Class 2 - Precision: 0.9769, Recall: 0.9852, F1-score: 0.9810\n",
    "#Class 3 - Precision: 0.9582, Recall: 0.9718, F1-score: 0.9650\n",
    "#Avg Precision: 0.9739, Avg Recall: 0.9731, Avg F1-score: 0.9734"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
